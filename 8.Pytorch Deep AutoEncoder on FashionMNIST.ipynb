{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### **AutoEncoder with Pytorch**\r\n",
    "\r\n",
    "### **What is AutoEncoder?**\r\n",
    "- AutoEncoder is type of Neural Network utilized to extract efficient features from unlabel data. It is Unsupervised Deep learning Architecture.\r\n",
    "- AutoEncoder is used to learn compresssed representation of data that used to learn to reconstruct the images from latent code space.\r\n",
    "- AutoEncoders obtains the latent code from a **Encoder Network**.\r\n",
    "- AutoEncoder Obtain the latent code as input to the **Decoder Network**, which tries to reconstruct the images that network has been trained on.\r\n",
    "> _â€œ**Autoencoding** is a data compression algorithm where the compression and decompression functions are \r\n",
    "1) data-specific, \r\n",
    "2) lossy, and \r\n",
    "3) learned automatically from examples rather than engineered by a human.\"_\r\n",
    "--  _Keras Blog_\r\n",
    "\r\n",
    "![](https://debuggercafe.com/wp-content/uploads/2019/12/autoencoder_nn-1-e1577235910280.png)\r\n",
    "\r\n",
    "### **Principal Behind AutoEncoder**\r\n",
    "- It Contains two part **Encoder** and **Decoder**.\r\n",
    "1. Encoder takes the input(x) and encodes($f(x)$) it.\r\n",
    "$$ r = f(x) $$\r\n",
    "2. Hidden Layer is work as bridge between **Encoder** and **Decoder**.\r\n",
    "$$ h = g(f(x)) $$\r\n",
    "3. Decoder tries to reconstruct the input data from hidden layer coding Decoding function($G$).\r\n",
    "$$ r = g(h) $$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Dataset: FashionMNIST  \r\n",
    "* **Train Images** : 60000   \r\n",
    "* **Test Images** : 10000  \r\n",
    "* **Classes** : 0: Tshirt/Top, 1: trouser, 2:pullover, 3:dress, 4:coat, 5: sandal, 6:shirt, 7: sneaker, 8: bag, 9:ankle boot  \r\n",
    "### **1. Load Libraries** \r\n",
    "\r\n",
    "Some of the important imports include:\r\n",
    "\r\n",
    "* `torchvision`: contains many popular computer vision datasets, deep neural network architectures, and image processing modules. We will use this to download the Fashion MNIST and in later articles the CIFAR10 dataset as well.\r\n",
    "* `torch.nn`: contains the deep learning neural network layers such as `Linear()`, and `Conv2d()`.\r\n",
    "* `transforms`: will help in defining the image transforms and normalizations.\r\n",
    "* `optim`: contains the deep learning optimizer classes such as `MSELoss()` and many others as well.\r\n",
    "* `functional`: we will use this for activation functions such as ReLU.\r\n",
    "* `DataLoader`: eases the task of making iterable training and testing sets."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "# torch loaded...!!!\r\n",
    "import torch\r\n",
    "import torch.nn.functional as F\r\n",
    "import torch.nn as nn\r\n",
    "import torch.optim as optim\r\n",
    "\r\n",
    "# torchvision loaded...!!!\r\n",
    "from torchvision import datasets, transforms\r\n",
    "from torchvision.utils import save_image\r\n",
    "from torchsummary import summary\r\n",
    "\r\n",
    "# other libraries loaded...!!!\r\n",
    "import os\r\n",
    "import numpy as np\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "%matplotlib inline\r\n",
    "import time"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### **2.Define the Constants**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# constants\r\n",
    "NUM_EPOCHS = 50\r\n",
    "LEARNING_RATE = 1e-3\r\n",
    "BATCH_SIZE = 128\r\n",
    "\r\n",
    "# image tranformation\r\n",
    "transform = transforms.Compose([transforms.ToTensor(),])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### **3.Download dataset and Load into DataLoader**\r\n",
    "\r\n",
    "- The `trainloader` and `testloader` each is of batch size `128`. The data loaders are iterable and you will be a1ble to iterate through them till the number of batches in them. Specifically, the `trainloader` contains `60000/128` number of batches, and the `testloader` contains `10000/128` number of batches."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "trainset = datasets.FashionMNIST(root=\"./data_new\", train=True, transform=transform, download=True)\r\n",
    "testset = datasets.FashionMNIST(root=\"./data_new\", train=False, transform=transform, download=True)\r\n",
    "\r\n",
    "trainLoader = torch.utils.data.DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True)\r\n",
    "testLoader = torch.utils.data.DataLoader(testset, batch_size=BATCH_SIZE, shuffle=False)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to ./data_new\\FashionMNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "26422272it [00:05, 4830894.61it/s]                              \n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Extracting ./data_new\\FashionMNIST\\raw\\train-images-idx3-ubyte.gz to ./data_new\\FashionMNIST\\raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to ./data_new\\FashionMNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "29696it [00:00, 216241.55it/s]                          \n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Extracting ./data_new\\FashionMNIST\\raw\\train-labels-idx1-ubyte.gz to ./data_new\\FashionMNIST\\raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to ./data_new\\FashionMNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "4422656it [00:01, 2943952.24it/s]                             \n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Extracting ./data_new\\FashionMNIST\\raw\\t10k-images-idx3-ubyte.gz to ./data_new\\FashionMNIST\\raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to ./data_new\\FashionMNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "6144it [00:00, 6175366.35it/s]          "
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Extracting ./data_new\\FashionMNIST\\raw\\t10k-labels-idx1-ubyte.gz to ./data_new\\FashionMNIST\\raw\n",
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n",
      "C:\\Users\\SHIVA\\miniconda3\\envs\\pytorch19\\lib\\site-packages\\torchvision\\datasets\\mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ..\\torch\\csrc\\utils\\tensor_numpy.cpp:180.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### **4.Utility Function**\r\n",
    "\r\n",
    "- The first function, `get_device()` either `returns the GPU device` if it is available or the `CPU`. If you notice, this is a bit different from the one-liner code used in the PyTorch tutorials. This is because some IDEs do not recognize the `torch.device()` method. Therefore, to keep the code compatible for both IDE and python notebooks I just changed the code a bit.\r\n",
    "\r\n",
    "- The second function is `make_dir()` which makes a directory to store the reconstructed images while training. At last, we have `save_decoded_image()` which saves the images that the autoencoder reconstructs."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# utility functions\r\n",
    "def get_device():\r\n",
    "    if torch.cuda.is_available():\r\n",
    "        device = torch.device(\"cuda:0\")\r\n",
    "    else:\r\n",
    "        device = torch.device(\"cpu\")\r\n",
    "    return device\r\n",
    "\r\n",
    "# make dir\r\n",
    "def make_dir():\r\n",
    "    image_dir = \"./FashionMNIST_Images\"\r\n",
    "    if not os.path.exists(image_dir):\r\n",
    "        os.mkdir(image_dir)\r\n",
    "        \r\n",
    "# save decoded images\r\n",
    "def save_decoded_image(img, epoch):\r\n",
    "    img = img.view(img.size(0), 1, 28, 28)\r\n",
    "    save_image(img, \"./FashionMNIST_Images/epoch_\" + str(epoch) + \".png\")\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### **5.Define AutoEncoder**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "    \r\n",
    "class AutoEncoder(nn.Module):\r\n",
    "    def __init__(self):\r\n",
    "        super(AutoEncoder, self).__init__()\r\n",
    "        \r\n",
    "        # encoder\r\n",
    "        self.efc1 = nn.Linear(784, 256)\r\n",
    "        self.efc2 = nn.Linear(256, 128)\r\n",
    "        self.efc3 = nn.Linear(128, 64)\r\n",
    "        self.efc4 = nn.Linear(64, 32)\r\n",
    "        self.efc5 = nn.Linear(32, 16)\r\n",
    "        \r\n",
    "        # decoder\r\n",
    "        self.dec1 = nn.Linear(16, 32)\r\n",
    "        self.dec2 = nn.Linear(32, 64)\r\n",
    "        self.dec3 = nn.Linear(64, 128)\r\n",
    "        self.dec4 = nn.Linear(128, 256)\r\n",
    "        self.dec5 = nn.Linear(256, 784)\r\n",
    "        \r\n",
    "    def forward(self, x):\r\n",
    "        # Encoder Module\r\n",
    "        x = F.relu(self.efc1(x))\r\n",
    "        x = F.relu(self.efc2(x))\r\n",
    "        x = F.relu(self.efc3(x))\r\n",
    "        x = F.relu(self.efc4(x))\r\n",
    "        x = F.relu(self.efc5(x))\r\n",
    "        \r\n",
    "        # Decoder Module\r\n",
    "        x = F.relu(self.dec1(x))\r\n",
    "        x = F.relu(self.dec2(x))\r\n",
    "        x = F.relu(self.dec3(x))\r\n",
    "        x = F.relu(self.dec4(x))\r\n",
    "        x = F.relu(self.dec5(x))\r\n",
    "        return x\r\n",
    "        "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "net = AutoEncoder()\r\n",
    "net.to(get_device())     "
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "AutoEncoder(\n",
       "  (efc1): Linear(in_features=784, out_features=256, bias=True)\n",
       "  (efc2): Linear(in_features=256, out_features=128, bias=True)\n",
       "  (efc3): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (efc4): Linear(in_features=64, out_features=32, bias=True)\n",
       "  (efc5): Linear(in_features=32, out_features=16, bias=True)\n",
       "  (dec1): Linear(in_features=16, out_features=32, bias=True)\n",
       "  (dec2): Linear(in_features=32, out_features=64, bias=True)\n",
       "  (dec3): Linear(in_features=64, out_features=128, bias=True)\n",
       "  (dec4): Linear(in_features=128, out_features=256, bias=True)\n",
       "  (dec5): Linear(in_features=256, out_features=784, bias=True)\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "summary(net, (784, ))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "â”œâ”€Linear: 1-1                            [-1, 256]                 200,960\n",
      "â”œâ”€Linear: 1-2                            [-1, 128]                 32,896\n",
      "â”œâ”€Linear: 1-3                            [-1, 64]                  8,256\n",
      "â”œâ”€Linear: 1-4                            [-1, 32]                  2,080\n",
      "â”œâ”€Linear: 1-5                            [-1, 16]                  528\n",
      "â”œâ”€Linear: 1-6                            [-1, 32]                  544\n",
      "â”œâ”€Linear: 1-7                            [-1, 64]                  2,112\n",
      "â”œâ”€Linear: 1-8                            [-1, 128]                 8,320\n",
      "â”œâ”€Linear: 1-9                            [-1, 256]                 33,024\n",
      "â”œâ”€Linear: 1-10                           [-1, 784]                 201,488\n",
      "==========================================================================================\n",
      "Total params: 490,208\n",
      "Trainable params: 490,208\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 0.49\n",
      "==========================================================================================\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.01\n",
      "Params size (MB): 1.87\n",
      "Estimated Total Size (MB): 1.89\n",
      "==========================================================================================\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "â”œâ”€Linear: 1-1                            [-1, 256]                 200,960\n",
       "â”œâ”€Linear: 1-2                            [-1, 128]                 32,896\n",
       "â”œâ”€Linear: 1-3                            [-1, 64]                  8,256\n",
       "â”œâ”€Linear: 1-4                            [-1, 32]                  2,080\n",
       "â”œâ”€Linear: 1-5                            [-1, 16]                  528\n",
       "â”œâ”€Linear: 1-6                            [-1, 32]                  544\n",
       "â”œâ”€Linear: 1-7                            [-1, 64]                  2,112\n",
       "â”œâ”€Linear: 1-8                            [-1, 128]                 8,320\n",
       "â”œâ”€Linear: 1-9                            [-1, 256]                 33,024\n",
       "â”œâ”€Linear: 1-10                           [-1, 784]                 201,488\n",
       "==========================================================================================\n",
       "Total params: 490,208\n",
       "Trainable params: 490,208\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 0.49\n",
       "==========================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 0.01\n",
       "Params size (MB): 1.87\n",
       "Estimated Total Size (MB): 1.89\n",
       "=========================================================================================="
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### **6. Loss function and Optimizer**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "# Loss function\r\n",
    "loss_fn = nn.MSELoss()\r\n",
    "\r\n",
    "# Optimizer function\r\n",
    "optimizer = optim.Adam(net.parameters(), lr=LEARNING_RATE)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### **7.Train and Test Function**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "def train(net, trainloader, NUM_EPOCHS):\r\n",
    "    train_loss = []\r\n",
    "    s_t = time.time()\r\n",
    "    for epoch in range(NUM_EPOCHS):\r\n",
    "        start = time.time()\r\n",
    "        running_loss = 0.0\r\n",
    "        for data in trainloader:\r\n",
    "            img, _ = data\r\n",
    "            img = img.to(device)\r\n",
    "            img = img.view(img.size(0), -1)\r\n",
    "            optimizer.zero_grad()\r\n",
    "            outputs = net(img)\r\n",
    "            loss = loss_fn(outputs, img)\r\n",
    "            loss.backward()\r\n",
    "            optimizer.step()\r\n",
    "            running_loss += loss.item()\r\n",
    "        \r\n",
    "        loss = running_loss / len(trainloader)\r\n",
    "        train_loss.append(loss)\r\n",
    "        end = time.time()\r\n",
    "        total_time = end - start\r\n",
    "        print('Epoch {} of {}, ETA : {:.2f} Seconds, Train Loss: {:.3f}'.format(\r\n",
    "                epoch+1, NUM_EPOCHS, total_time, loss))\r\n",
    "        if epoch % 5 == 0:\r\n",
    "            save_decoded_image(outputs.cpu().data, epoch)\r\n",
    "    e_t = time.time()\r\n",
    "    print(\"Total time : {:.2f} Minutes\".format((e_t - s_t)/60))\r\n",
    "    return train_loss\r\n",
    "\r\n",
    "def test_image_reconstruction(net, testloader):\r\n",
    "     for batch in testloader:\r\n",
    "        img, _ = batch\r\n",
    "        img = img.to(device)\r\n",
    "        img = img.view(img.size(0), -1)\r\n",
    "        outputs = net(img)\r\n",
    "        outputs = outputs.view(outputs.size(0), 1, 28, 28).cpu().data\r\n",
    "        save_image(outputs, 'fashionmnist_reconstruction.png')\r\n",
    "        break"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### **8.Train AutoEncoder**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "# get the computation device\r\n",
    "device = get_device()\r\n",
    "print(device)\r\n",
    "\r\n",
    "# load the neural network onto the device\r\n",
    "net.to(device)\r\n",
    "make_dir()\r\n",
    "\r\n",
    "# train the network\r\n",
    "train_loss = train(net, trainLoader, NUM_EPOCHS)\r\n",
    "plt.figure()\r\n",
    "plt.plot(train_loss)\r\n",
    "plt.title('Train Loss')\r\n",
    "plt.xlabel('Epochs')\r\n",
    "plt.ylabel('Loss')\r\n",
    "plt.savefig('deep_ae_fashionmnist_loss.png')\r\n",
    "\r\n",
    "# test the network\r\n",
    "test_image_reconstruction(net, testLoader)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "cuda:0\n",
      "Epoch 1 of 50, ETA : 9.83 Seconds, Train Loss: 0.074\n",
      "Epoch 2 of 50, ETA : 10.03 Seconds, Train Loss: 0.041\n",
      "Epoch 3 of 50, ETA : 9.62 Seconds, Train Loss: 0.033\n",
      "Epoch 4 of 50, ETA : 9.69 Seconds, Train Loss: 0.029\n",
      "Epoch 5 of 50, ETA : 9.77 Seconds, Train Loss: 0.028\n",
      "Epoch 6 of 50, ETA : 9.75 Seconds, Train Loss: 0.026\n",
      "Epoch 7 of 50, ETA : 9.82 Seconds, Train Loss: 0.025\n",
      "Epoch 8 of 50, ETA : 10.04 Seconds, Train Loss: 0.024\n",
      "Epoch 9 of 50, ETA : 9.74 Seconds, Train Loss: 0.023\n",
      "Epoch 10 of 50, ETA : 9.72 Seconds, Train Loss: 0.023\n",
      "Epoch 11 of 50, ETA : 9.75 Seconds, Train Loss: 0.022\n",
      "Epoch 12 of 50, ETA : 9.75 Seconds, Train Loss: 0.022\n",
      "Epoch 13 of 50, ETA : 9.71 Seconds, Train Loss: 0.021\n",
      "Epoch 14 of 50, ETA : 12.85 Seconds, Train Loss: 0.021\n",
      "Epoch 15 of 50, ETA : 8673.00 Seconds, Train Loss: 0.021\n",
      "Epoch 16 of 50, ETA : 29.05 Seconds, Train Loss: 0.021\n",
      "Epoch 17 of 50, ETA : 27.66 Seconds, Train Loss: 0.020\n",
      "Epoch 18 of 50, ETA : 19.47 Seconds, Train Loss: 0.020\n",
      "Epoch 19 of 50, ETA : 18.94 Seconds, Train Loss: 0.020\n",
      "Epoch 20 of 50, ETA : 19.75 Seconds, Train Loss: 0.020\n",
      "Epoch 21 of 50, ETA : 19.08 Seconds, Train Loss: 0.020\n",
      "Epoch 22 of 50, ETA : 18.73 Seconds, Train Loss: 0.020\n",
      "Epoch 23 of 50, ETA : 19.20 Seconds, Train Loss: 0.020\n",
      "Epoch 24 of 50, ETA : 19.51 Seconds, Train Loss: 0.020\n",
      "Epoch 25 of 50, ETA : 19.58 Seconds, Train Loss: 0.019\n",
      "Epoch 26 of 50, ETA : 19.57 Seconds, Train Loss: 0.019\n",
      "Epoch 27 of 50, ETA : 19.55 Seconds, Train Loss: 0.019\n",
      "Epoch 28 of 50, ETA : 19.52 Seconds, Train Loss: 0.019\n",
      "Epoch 29 of 50, ETA : 19.40 Seconds, Train Loss: 0.019\n",
      "Epoch 30 of 50, ETA : 19.40 Seconds, Train Loss: 0.019\n",
      "Epoch 31 of 50, ETA : 19.14 Seconds, Train Loss: 0.019\n",
      "Epoch 32 of 50, ETA : 18.91 Seconds, Train Loss: 0.019\n",
      "Epoch 33 of 50, ETA : 19.16 Seconds, Train Loss: 0.019\n",
      "Epoch 34 of 50, ETA : 20.39 Seconds, Train Loss: 0.019\n",
      "Epoch 35 of 50, ETA : 17.94 Seconds, Train Loss: 0.018\n",
      "Epoch 36 of 50, ETA : 17.08 Seconds, Train Loss: 0.018\n",
      "Epoch 37 of 50, ETA : 16.67 Seconds, Train Loss: 0.018\n",
      "Epoch 38 of 50, ETA : 16.87 Seconds, Train Loss: 0.018\n",
      "Epoch 39 of 50, ETA : 20.34 Seconds, Train Loss: 0.018\n",
      "Epoch 40 of 50, ETA : 16.76 Seconds, Train Loss: 0.018\n",
      "Epoch 41 of 50, ETA : 17.96 Seconds, Train Loss: 0.018\n",
      "Epoch 42 of 50, ETA : 16.54 Seconds, Train Loss: 0.018\n",
      "Epoch 43 of 50, ETA : 16.91 Seconds, Train Loss: 0.017\n",
      "Epoch 44 of 50, ETA : 16.76 Seconds, Train Loss: 0.017\n",
      "Epoch 45 of 50, ETA : 17.36 Seconds, Train Loss: 0.017\n",
      "Epoch 46 of 50, ETA : 16.67 Seconds, Train Loss: 0.017\n",
      "Epoch 47 of 50, ETA : 16.95 Seconds, Train Loss: 0.017\n",
      "Epoch 48 of 50, ETA : 16.65 Seconds, Train Loss: 0.017\n",
      "Epoch 49 of 50, ETA : 17.32 Seconds, Train Loss: 0.017\n",
      "Epoch 50 of 50, ETA : 15.09 Seconds, Train Loss: 0.017\n",
      "Total time : 157.89 Minutes\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.0",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.0 64-bit ('pytorch19': conda)"
  },
  "interpreter": {
   "hash": "884a8f81666a19c0851426c83cd6eaa7b212468ad852fb3caa21591c98d4369f"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}